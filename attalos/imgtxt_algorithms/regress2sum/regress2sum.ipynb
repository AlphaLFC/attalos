{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "import gzip\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix, csr_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "# Attalos Imports\n",
    "import attalos.util.log.log as l\n",
    "from attalos.dataset.dataset import Dataset\n",
    "from attalos.evaluation.evaluation import Evaluation\n",
    "\n",
    "# Local models\n",
    "from mse import MSEModel\n",
    "from negsampling import NegSamplingModel\n",
    "from fast0tag import FastZeroTagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup global objects\n",
    "logger = l.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from attalos.imgtxt_algorithms.regress2sum.multihot import MultihotModel\n",
    "from attalos.imgtxt_algorithms.regress2sum.naivesum import NaiveSumModel\n",
    "from attalos.imgtxt_algorithms.regress2sum.wdv import WDVModel\n",
    "from attalos.dataset.wordvectors.glove import GloveWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Temp object using duck typing to replace command line arguments\n",
    "args = lambda: None\n",
    "#args.image_feature_file_train = \"/local_data/teams/attalos/features/image/espgame_train_20160823_inception.hdf5\"\n",
    "#args.text_feature_file_train = \"/local_data/teams/attalos/features/text/espgame_train_20160823_text.json.gz\"\n",
    "#args.image_feature_file_test = \"/local_data/teams/attalos/features/image/espgame_test_20160823_inception.hdf5\"\n",
    "#args.text_feature_file_test = \"/local_data/teams/attalos/features/text/espgame_test_20160823_text.json.gz\"\n",
    "args.image_feature_file_train = \"/local_data/teams/attalos/features/image/iaprtc_train_20160816_inception.hdf5\"\n",
    "args.text_feature_file_train = \"/local_data/teams/attalos/features/text/iaprtc_train_20160816_text.json.gz\"\n",
    "args.image_feature_file_test = \"/local_data/teams/attalos/features/image/iaprtc_test_20160816_inception.hdf5\"\n",
    "args.text_feature_file_test = \"/local_data/teams/attalos/features/text/iaprtc_test_20160816_text.json.gz\"\n",
    "args.word_vector_file = \"/local_data/kylez/glove.6B.200d.txt\"\n",
    "args.word_vector_type = \"glove\"\n",
    "args.model_type = \"wdv\"\n",
    "args.cross_eval = False\n",
    "args.in_memory = True\n",
    "args.model_input_path = None\n",
    "args.model_output_path = None\n",
    "args.num_epochs = 400\n",
    "args.batch_size = 100\n",
    "args.learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordVectorTypes(Enum):\n",
    "    w2v = 1\n",
    "    glove = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelTypes(Enum):\n",
    "    mse = 1\n",
    "    negsampling = 2\n",
    "    fast0tag = 3\n",
    "    multihot = MultihotModel\n",
    "    naivesum = NaiveSumModel\n",
    "    wdv = WDVModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(sess, model, batch):\n",
    "    train_x, train_y = batch\n",
    "    training_loss = model.fit(sess, train_x, train_y)\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(sess, model, train_dataset, batch_size):\n",
    "    training_losses = []\n",
    "    for cur_batch_num, batch in enumerate(model.to_batches(train_dataset, batch_size)):\n",
    "        training_loss = train_batch(sess, model, batch)\n",
    "        training_losses.append(training_loss)\n",
    "    avg_training_loss = sum(training_losses) / float(len(training_losses))\n",
    "    return avg_training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, model, num_epochs, train_dataset, batch_size, epoch_verbosity_rate=10):\n",
    "    for cur_epoch in xrange(num_epochs):\n",
    "        verbose = cur_epoch % epoch_verbosity_rate == 0\n",
    "        avg_training_loss = train_epoch(sess, model, train_dataset, batch_size)\n",
    "        if verbose:\n",
    "            logger.info(\"Finished epoch %s. (Avg. training loss: %s)\" % (cur_epoch, avg_training_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_wv_model(word_vector_file, word_vector_type):\n",
    "    if args.word_vector_type == WordVectorTypes.glove.name:\n",
    "        from glove import Glove\n",
    "        glove_model = Glove.load_stanford(word_vector_file)\n",
    "        wv_model = GloveWrapper(glove_model)\n",
    "    else: #args.word_vector_type == WordVectorTypes.w2v.name:\n",
    "        import word2vec\n",
    "        w2v_model = word2vec.load(word_vector_file)\n",
    "        wv_model = W2VWrapper(w2v_model)\n",
    "    return wv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-09-09 19:37:07,239] [INFO] Parsing train and test datasets.\n",
      "[2016-09-09 19:37:07,557] [INFO] Reading word vectors from file.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Parsing train and test datasets.\")\n",
    "train_dataset = Dataset(args.image_feature_file_train, args.text_feature_file_train, load_image_feats_in_mem=args.in_memory)\n",
    "test_dataset = Dataset(args.image_feature_file_test, args.text_feature_file_test)\n",
    "\n",
    "logger.info(\"Reading word vectors from file.\")\n",
    "wv_model = load_wv_model(args.word_vector_file, args.word_vector_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-09-09 19:42:14,013] [INFO] Selecting model class: WDVModel\n",
      "[2016-09-09 19:42:18,153] [DEBUG] Constructing w1.\n",
      "[2016-09-09 19:42:25,909] [DEBUG] No specified vocab2. Setting w2 = w1.\n",
      "[2016-09-09 19:42:25,912] [DEBUG] Multiplying w1 and w2.\n",
      "[2016-09-09 19:42:25,916] [DEBUG] Applying preprocess_fn.\n",
      "[2016-09-09 19:42:26,021] [INFO] Input size: 2048\n",
      "[2016-09-09 19:42:26,022] [INFO] Output size: 288\n",
      "[2016-09-09 19:42:26,025] [INFO] Hidden layer size: 1168\n",
      "[2016-09-09 19:42:26,041] [INFO] Hidden layer size: 1168\n",
      "[2016-09-09 19:42:26,069] [INFO] Hidden layer size: 1168\n"
     ]
    }
   ],
   "source": [
    "model_cls = ModelTypes[args.model_type].value\n",
    "logger.info(\"Selecting model class: %s\" % model_cls.__name__)\n",
    "#datasets = [train_dataset] if args.cross_eval else [train_dataset, test_dataset]\n",
    "model = model_cls(wv_model, train_dataset=train_dataset, test_dataset=test_dataset, **vars(args))\n",
    "model.initialize_model(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-09-09 19:42:27,778] [INFO] Starting training phase.\n",
      "[2016-09-09 19:43:00,889] [INFO] Finished epoch 0. (Avg. training loss: 148.447199865)\n",
      "[2016-09-09 19:43:47,840] [INFO] Finished epoch 10. (Avg. training loss: 84.4973350005)\n",
      "[2016-09-09 19:44:12,841] [INFO] Finished epoch 20. (Avg. training loss: 77.1428703395)\n",
      "[2016-09-09 19:44:37,677] [INFO] Finished epoch 30. (Avg. training loss: 70.1581770724)\n",
      "[2016-09-09 19:45:02,589] [INFO] Finished epoch 40. (Avg. training loss: 63.6083343679)\n",
      "[2016-09-09 19:45:27,424] [INFO] Finished epoch 50. (Avg. training loss: 57.3654346466)\n",
      "[2016-09-09 19:45:52,148] [INFO] Finished epoch 60. (Avg. training loss: 51.9774155834)\n",
      "[2016-09-09 19:46:16,772] [INFO] Finished epoch 70. (Avg. training loss: 47.4211089394)\n",
      "[2016-09-09 19:46:41,418] [INFO] Finished epoch 80. (Avg. training loss: 43.7574630434)\n",
      "[2016-09-09 19:47:05,984] [INFO] Finished epoch 90. (Avg. training loss: 40.6061665362)\n",
      "[2016-09-09 19:47:30,622] [INFO] Finished epoch 100. (Avg. training loss: 38.0673392252)\n",
      "[2016-09-09 19:47:55,203] [INFO] Finished epoch 110. (Avg. training loss: 35.9420087771)\n",
      "[2016-09-09 19:48:19,998] [INFO] Finished epoch 120. (Avg. training loss: 34.2185067805)\n",
      "[2016-09-09 19:48:45,173] [INFO] Finished epoch 130. (Avg. training loss: 32.5745079517)\n",
      "[2016-09-09 19:49:10,628] [INFO] Finished epoch 140. (Avg. training loss: 31.3100394552)\n",
      "[2016-09-09 19:49:35,564] [INFO] Finished epoch 150. (Avg. training loss: 30.2041136893)\n",
      "[2016-09-09 19:50:00,309] [INFO] Finished epoch 160. (Avg. training loss: 29.1901725097)\n",
      "[2016-09-09 19:50:25,066] [INFO] Finished epoch 170. (Avg. training loss: 28.2223943689)\n",
      "[2016-09-09 19:50:49,680] [INFO] Finished epoch 180. (Avg. training loss: 27.4360638315)\n",
      "[2016-09-09 19:51:14,613] [INFO] Finished epoch 190. (Avg. training loss: 26.9947614887)\n",
      "[2016-09-09 19:51:39,810] [INFO] Finished epoch 200. (Avg. training loss: 26.4498278444)\n",
      "[2016-09-09 19:52:05,239] [INFO] Finished epoch 210. (Avg. training loss: 25.9491496411)\n",
      "[2016-09-09 19:52:30,263] [INFO] Finished epoch 220. (Avg. training loss: 25.496373079)\n",
      "[2016-09-09 19:52:55,188] [INFO] Finished epoch 230. (Avg. training loss: 24.8821093819)\n",
      "[2016-09-09 19:53:19,855] [INFO] Finished epoch 240. (Avg. training loss: 24.6430393349)\n",
      "[2016-09-09 19:53:44,620] [INFO] Finished epoch 250. (Avg. training loss: 24.1853265546)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b6925b5f7774>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting training phase.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, train_dataset, wv_model, test_dataset=test_dataset, epoch_verbosity_rate=100)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-2a4e3dffde82>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(sess, model, num_epochs, train_dataset, batch_size, epoch_verbosity_rate)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mepoch_verbosity_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mavg_training_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished epoch %s. (Avg. training loss: %s)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_training_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-99acce7af0d1>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(sess, model, train_dataset, batch_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtraining_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcur_batch_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mtraining_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mtraining_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mavg_training_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c53574bbbabb>\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(sess, model, batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtraining_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data/fs4/home/kylez/attalos/attalos/imgtxt_algorithms/regress2sum/wdv.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, sess, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y_truth\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dropout_keep_prob\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m                            })\n\u001b[0;32m    157\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 382\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 655\u001b[1;33m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 723\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    724\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    728\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting training phase.\")\n",
    "train(sess, model, args.num_epochs, train_dataset, args.batch_size) #, train_dataset, wv_model, test_dataset=test_dataset, epoch_verbosity_rate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-09-09 19:54:02,409] [INFO] Starting evaluation phase.\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[2016-09-09 19:54:02,965] [INFO] Evaluation (precision, recall, f1): [0.40860336819079413, 0.28596257579458823, 0.31335328330840467]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting evaluation phase.\")\n",
    "test_x, test_y = model.to_ndarrs(test_dataset)\n",
    "predictions = model.predict(sess, test_x)\n",
    "evaluator = Evaluation(test_y, predictions, k=5)\n",
    "logger.info(\"Evaluation (precision, recall, f1): %s\" % evaluator.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
